import pandas as pd
import numpy as np
from pandas import DataFrame
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn import metrics
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_curve #导入ROC曲线函数
import matplotlib.pyplot as plt
import os
import graphviz
from sklearn import tree

data1 = pd.read_csv('C:/Users/Chilu/Desktop/DISSdata/supervised/original.csv')

col=['IncType','IncType2','IncType3','PropertyTypeLevel1','CallOrigin','MobiliseIncidentType','PropertyCategorySummary','ServiceDeliveryArea','Pumps','Fatalities','SeriousInjuries','SlightInjuries','FirstAidInjury','PrecautionaryCheck','RescueOnly','rainfallRegion','Label']
data1=DataFrame(data1,columns=col)
data1['IncType']=pd.Categorical(data1['IncType']).codes
data1['IncType2']=pd.Categorical(data1['IncType2']).codes
data1['IncType3']=pd.Categorical(data1['IncType3']).codes
data1['PropertyTypeLevel1']=pd.Categorical(data1['PropertyTypeLevel1']).codes
data1['CallOrigin']=pd.Categorical(data1['CallOrigin']).codes
data1['MobiliseIncidentType']=pd.Categorical(data1['MobiliseIncidentType']).codes
data1['PropertyCategorySummary']=pd.Categorical(data1['PropertyCategorySummary']).codes
data1['ServiceDeliveryArea']=pd.Categorical(data1['ServiceDeliveryArea']).codes
data1['Label']=pd.Categorical(data1['Label']).codes
#data1=DataFrame(data1,dtype=np.int64)
X=data1.iloc[:,0:16].values
y=data1.iloc[:,16].values

#split into test and training set
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=100)

#build random forest
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100,max_depth=4,min_samples_split=10,oob_score=True,bootstrap=True,n_jobs=-1 )
clf = clf.fit(X_train,y_train)

#accuracy 
print('100棵树:',clf.score(X_test,y_test))

#predict random forest 
y_pred=clf.predict(X_test)

#AUC
test_auc = metrics.roc_auc_score(y_test,y_pred)#验证集上的auc值
print('test auc',test_auc)

#cross validation 
result = cross_val_score(clf, X, y, cv=10)
print('10-fold accuracy:',result.mean())

#ROC
fpr,tpr,threshold = roc_curve(y_test, y_pred) ###计算真正率和假正率
roc_auc = auc(fpr,tpr)

#plot roc curve
plt.figure()
lw = 2
plt.figure(figsize=(10,10))
plt.plot(fpr, tpr, color='darkorange',
         lw=lw, label='ROC curve (area = %0.4f)' % roc_auc) ###假正率为横坐标，真正率为纵坐标做曲线
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Random Forest')
plt.legend(loc="lower right")
plt.show()

#confusion matrix
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix

def cm_plot(y, yp):
    cm = confusion_matrix(y, yp)
    plt.matshow(cm, cmap=plt.cm.Blues)
    plt.colorbar()
        
    for x in range(len(cm)): 
        for y in range(len(cm)):
            plt.annotate(cm[x,y], xy=(x, y), horizontalalignment='center', verticalalignment='center')
            
    plt.ylabel('True label') 
    plt.xlabel('Predicted label') 
    return plt

cm_plot(y_test,y_pred).show()

vec = DictVectorizer()
data_vectorized = vec.fit_transform(list(data1.T.to_dict(orient='dict').values()))
vec.get_feature_names() #Shows feature names

Label = data1['Label']
clf = RandomForestClassifier()
clf = clf.fit(data_vectorized, Label)

data1.columns.tolist()[:-1]

#feature importance 
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.ensemble import ExtraTreesClassifier

# Build a forest and compute the feature importances
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators=100,n_jobs=-1,max_depth=4,oob_score=True,bootstrap=True,min_samples_split=10)
forest.fit(X, y)
importances = forest.feature_importances_
std = np.std([tree.feature_importances_ for tree in forest.estimators_],
             axis=0)
indices = np.argsort(importances)[::-1]

# Print the feature ranking
print("Feature ranking:")

for f in range(X_train.shape[1]):
    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))

# Plot the feature importances of the forest
plt.figure()
plt.title("Feature importances")
plt.bar(range(X_train.shape[1]), importances[indices],
       color="orange", yerr=std[indices], align="center")
plt.xticks(range(X_train.shape[1]), indices)
plt.xlim([-1, X_train.shape[1]])
plt.show()

#tunning n_estimators
n_estimators = [2,5,10,15,20,25,30,35,40,45,50,60,80,100]
train_results = []
test_results = []
for estimator in n_estimators:
   rf = RandomForestClassifier(n_estimators=estimator,n_jobs=-1,max_depth=4,oob_score=True,bootstrap=True,min_samples_split=10) 
   rf.fit(X_train, y_train)
   train_pred = rf.predict(X_train)
   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)
   roc_auc = auc(false_positive_rate, true_positive_rate)
   train_results.append(roc_auc)
   y_pred = rf.predict(X_test)
   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)
   roc_auc = auc(false_positive_rate, true_positive_rate)
   test_results.append(roc_auc)

from matplotlib.legend_handler import HandlerLine2D
line1, = plt.plot(n_estimators, train_results, 'b', label='Train AUC')
line2, = plt.plot(n_estimators, test_results, 'r', label='Test AUC')
plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})
plt.ylabel('AUC socre')
plt.xlabel('n_estimators')
plt.show()

#grid search 
#再对内部节点再划分所需最小样本数min_samples_split和叶子节点最少样本数min_samples_leaf一起调参
from sklearn.model_selection import GridSearchCV
param_test3= {'min_samples_split':range(80,150,20), 'min_samples_leaf':range(10,60,10)}
gsearch3= GridSearchCV(estimator = RandomForestClassifier(n_estimators= 60,max_depth=13,
                                 max_features='sqrt' ,oob_score=True, random_state=10),
   param_grid = param_test3,scoring='roc_auc',iid=False, cv=10)
gsearch3.fit(X,y)
gsearch3.grid_scores_,gsearch2.best_params_, gsearch2.best_score_

#原文链接：https://blog.csdn.net/CherDW/article/details/54971771

#Out Of Bag score 
rf1 = RandomForestClassifier(n_estimators=100,n_jobs=-1,max_depth=4,oob_score=True,bootstrap=True,min_samples_split=10)#,random_state=10
rf1.fit(X,y)
print (rf1.oob_score_)
